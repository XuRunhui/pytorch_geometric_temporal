# PyTorch Lightning Trainer configuration for DDP GPU training
# @package trainer

# Basic training settings
max_epochs: 200
min_epochs: 10
check_val_every_n_epoch: 1

# Hardware settings
accelerator: "gpu"
devices: 2
strategy: "ddp"
precision: 32  # Mixed precision for faster training

# Performance optimizations
benchmark: true  # Optimize for consistent input sizes
deterministic: false  # Set to true for reproducible results (slower)

# Training behavior
gradient_clip_val: 1.0
gradient_clip_algorithm: "norm"
accumulate_grad_batches: 1

# Logging and monitoring
log_every_n_steps: 1
enable_progress_bar: true
enable_model_summary: true

# Validation and testing
limit_train_batches: 1.0  # Use full dataset, set to smaller value for debugging
limit_val_batches: 1.0
limit_test_batches: 1.0
val_check_interval: 1.0  # Check validation every epoch

# Checkpointing
enable_checkpointing: true
default_root_dir: null  # Will be set by hydra

# DDP specific settings
sync_batchnorm: true
find_unused_parameters: false

# Debugging and profiling
fast_dev_run: false  # Set to true for quick debugging
overfit_batches: 0  # Set to small number to test overfitting
detect_anomaly: false  # Enable for debugging NaN/Inf

# Early stopping patience (if using early stopping callback)
patience: 10
